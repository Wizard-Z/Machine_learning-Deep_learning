{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM in PyTorch.\n",
    "Character level RNN for generating text based on input on which it has been trained.\n",
    "Important links<br>\n",
    "- [Andrej Karpathy post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Exploring LSTM](http://blog.echen.me/2017/05/30/exploring-lstms/)\n",
    "- [Geoffrey Hinton ppts.](http://www.cs.toronto.edu/~hinton/talks.html)\n",
    "\n",
    "General architecture of the network:\n",
    "![Architecture](assets/charseq.jpeg \"RNN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tpx4XVi2lT1T"
   },
   "source": [
    "## 0. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atGLxz6qk3kF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "MRE2fnmmlX-o",
    "outputId": "e1eabfdb-c42d-44ce-9cb7-1065df0fbc40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every un\n"
     ]
    }
   ],
   "source": [
    "# Loading the nobel\n",
    "PATH = 'data/'\n",
    "with open(PATH+'anna.txt' ,'r') as f:\n",
    "  text = f.read()\n",
    "\n",
    "print(text[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXjeEPdUllIZ"
   },
   "source": [
    "## 1. Tokenize and make data data ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "V8JbZ2s6lhZ_",
    "outputId": "5ca75c46-aa7f-46d9-df3e-f0935fe8be34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char2int:\n",
      "{'E': 0, ':': 1, '4': 2, '@': 3, '$': 4, 'S': 5, '9': 6, 'm': 7, 'F': 8, 'T': 9, 'B': 10, 'y': 11, 'K': 12, 'b': 13, 'I': 14, '%': 15, '&': 16, '\"': 17, '\\n': 18, 'w': 19, '/': 20, 'Q': 21, 'V': 22, 'L': 23, 'u': 24, 'n': 25, 'o': 26, 'D': 27, 'X': 28, 'Z': 29, '!': 30, 't': 31, '?': 32, 'h': 33, 'r': 34, 'M': 35, 'O': 36, 'j': 37, 'H': 38, 'W': 39, '`': 40, 'a': 41, '-': 42, ' ': 43, 'G': 44, 'z': 45, 'C': 46, 'd': 47, 'J': 48, '6': 49, 'k': 50, '1': 51, 'c': 52, '0': 53, 'P': 54, 'N': 55, 'R': 56, '_': 57, '(': 58, 'Y': 59, ',': 60, 'g': 61, '2': 62, '7': 63, '*': 64, 'v': 65, 'U': 66, '.': 67, 'e': 68, 's': 69, 'l': 70, 'x': 71, 'q': 72, '5': 73, 'p': 74, ';': 75, '8': 76, '3': 77, 'f': 78, 'i': 79, \"'\": 80, 'A': 81, ')': 82}\n",
      "\n",
      "int2char:\n",
      "{0: 'E', 1: ':', 2: '4', 3: '@', 4: '$', 5: 'S', 6: '9', 7: 'm', 8: 'F', 9: 'T', 10: 'B', 11: 'y', 12: 'K', 13: 'b', 14: 'I', 15: '%', 16: '&', 17: '\"', 18: '\\n', 19: 'w', 20: '/', 21: 'Q', 22: 'V', 23: 'L', 24: 'u', 25: 'n', 26: 'o', 27: 'D', 28: 'X', 29: 'Z', 30: '!', 31: 't', 32: '?', 33: 'h', 34: 'r', 35: 'M', 36: 'O', 37: 'j', 38: 'H', 39: 'W', 40: '`', 41: 'a', 42: '-', 43: ' ', 44: 'G', 45: 'z', 46: 'C', 47: 'd', 48: 'J', 49: '6', 50: 'k', 51: '1', 52: 'c', 53: '0', 54: 'P', 55: 'N', 56: 'R', 57: '_', 58: '(', 59: 'Y', 60: ',', 61: 'g', 62: '2', 63: '7', 64: '*', 65: 'v', 66: 'U', 67: '.', 68: 'e', 69: 's', 70: 'l', 71: 'x', 72: 'q', 73: '5', 74: 'p', 75: ';', 76: '8', 77: '3', 78: 'f', 79: 'i', 80: \"'\", 81: 'A', 82: ')'}\n",
      "\n",
      "Length:1985223\n",
      "\n",
      "Encoded:\n",
      "[46 33 41 74 31 68 34 43 51 18 18 18 38 41 74 74 11 43 78 41  7 79 70 79\n",
      " 68 69 43 41 34 68 43 41 70 70 43 41 70 79 50 68 75 43 68 65 68 34 11 43\n",
      " 24 25]\n",
      "\n",
      "\n",
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "chars = tuple(set(text))\n",
    "# Dictionaries for mapping characters to integers and vice versa.\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {c:i for i,c in int2char.items()}\n",
    "# Encode entire text to numbers i.e each character with its numerical value.\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "print('char2int:\\n{}\\n\\nint2char:\\n{}\\n\\nLength:{}\\n\\nEncoded:\\n{}\\n\\n'.format(char2int,int2char,len(encoded),encoded[:50]))\n",
    "\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Reshape\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ayCs36wwTyc"
   },
   "source": [
    "## 2. Minibatches\n",
    "![Batches](assets/sequence_batching_ex.png \"Batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D75uQOTPwTgs"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "  '''\n",
    "  Generator which returns batch of size : batch_size*seq_length\n",
    "  '''\n",
    "  chars_batch = batch_size * seq_length\n",
    "  n_batches = len(arr) // chars_batch\n",
    "  \n",
    "  # Make full batches\n",
    "  arr = arr[: n_batches * chars_batch]\n",
    "  # Resize the array\n",
    "  arr = arr.reshape((batch_size,-1))\n",
    "  # Get Batches\n",
    "  for i in range(0, arr.shape[1], seq_length):\n",
    "    # Features\n",
    "    x = arr[:, i:i + seq_length]\n",
    "    # Targets shifted by One in future\n",
    "    y = np.zeros_like(x)\n",
    "    try:\n",
    "      y[:,:-1], y[:,-1] = x[:,1:], arr[:,i+seq_length]\n",
    "    except IndexError:\n",
    "      y[:,:-1], y[:,-1] = x[:,1:], arr[:,0]\n",
    "    \n",
    "    yield x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "colab_type": "code",
    "id": "Z8wr8wBjwTda",
    "outputId": "a96061d6-e1b4-4b41-e946-86d3bb10c2a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (8, 50)\n",
      "x\n",
      " [[82  7 17 47 65 62  8 63 41 57 57 57 56 17 47 47 74 63 59 17 67 51 16 51\n",
      "  62 52 63 17  8 62 63 17 16 16 63 17 16 51 40 62 75 63 62 66 62  8 74 63\n",
      "  76 70]\n",
      " [52 55 70 63 65  7 17 65 63 17 65 65  8 17 25 65 62 72 63  7 62  8 63 17\n",
      "  65 65 62 70 65 51 55 70 63 22 17 52 63  7 62  8 63  7 76 52 68 17 70 72\n",
      "  31 63]\n",
      " [62 70 72 63 55  8 63 17 63 59 55 62  3 63  7 62 63 17 66 55 51 72 62 72\n",
      "  63  7 51 52 63 59 17 65  7 62  8 31 63 56 62 57 16 55 55 40 62 72 63  8\n",
      "  55 76]\n",
      " [52 63 65  7 62 63 25  7 51 62 59 63 65  7 55 76 30  7 63  7 51 72 72 62\n",
      "  70 57 51 70 65 62  8 62 52 65 63 55 59 63  7 51 52 63 16 51 59 62  3 63\n",
      "  55 59]\n",
      " [63 52 17 22 63  7 62  8 63 65 62 17  8 27 52 65 17 51 70 62 72  3 63 47\n",
      "  51 65 51 59 76 16  3 63 52 22 62 62 65 63 59 17 25 62  3 57 67 51 52 62\n",
      "   8 17]\n",
      " [25 76 52 52 51 55 70 63 17 70 72 63 17 70 17 16 74 52 51 52  3 63 22 17\n",
      "  52 63 51 70 63 47  8 51 70 25 51 47 16 62 63 72 51 52 17 30  8 62 62 17\n",
      "  68 16]\n",
      " [63 79 70 70 17 63  7 17 72 63 52 17 51 72 63 65  7 17 65 63 49 55 16 16\n",
      "  74 63 22 55 76 16 72 63 62 18 25 76 52 62 63 51 65 31 63 79 70 72 63 65\n",
      "   7 51]\n",
      " [19 68 16 55 70 52 40 74 31 63 54  4 76 65 63 81 65  7 62 74 81 63 25 17\n",
      "  70 70 55 65 63 30  8 17 52 47 63 65  7 17 65  3 57 81 65  7 62 74 81 63\n",
      "  17  8]]\n",
      "\n",
      "y\n",
      " [[ 7 17 47 65 62  8 63 41 57 57]\n",
      " [55 70 63 65  7 17 65 63 17 65]\n",
      " [70 72 63 55  8 63 17 63 59 55]\n",
      " [63 65  7 62 63 25  7 51 62 59]\n",
      " [52 17 22 63  7 62  8 63 65 62]\n",
      " [76 52 52 51 55 70 63 17 70 72]\n",
      " [79 70 70 17 63  7 17 72 63 52]\n",
      " [68 16 55 70 52 40 74 31 63 54]]\n"
     ]
    }
   ],
   "source": [
    "# TEST RUN\n",
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n",
    "# printing out the first 10 items in a sequence\n",
    "print('Shape', x.shape)\n",
    "print('x\\n',x)\n",
    "print('\\ny\\n', y[:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Jx-iG1UewTYy",
    "outputId": "0092a3ec-2ad4-4124-89eb-295381b94995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO GPU | Training on CPU\n"
     ]
    }
   ],
   "source": [
    "# GPU check\n",
    "gpu_is = torch.cuda.is_available()\n",
    "if(gpu_is):\n",
    "  print('Training on GPU')\n",
    "else:\n",
    "  print('NO GPU | Training on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cikqumpq3KDN"
   },
   "source": [
    "## 3. Defining Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzKYGe3vwTU7"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "  def __init__(self, tokens, n_hidden = 256, n_layers = 2, drop_prob = 0.3, lr = 0.001):\n",
    "    super().__init__()\n",
    "    self.drop_prob = drop_prob\n",
    "    self.n_hidden = n_hidden\n",
    "    self.n_layers = n_layers\n",
    "    self.lr = lr\n",
    "    \n",
    "    # Character dictionaries\n",
    "    self.chars = tokens\n",
    "    self.int2char = dict(enumerate(self.chars))\n",
    "    self.char2int = {c:i for i,c in self.int2char.items()}\n",
    "    \n",
    "    # LSTM \n",
    "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout = drop_prob, batch_first = True)\n",
    "    # Dropout\n",
    "    self.dropout = nn.Dropout(drop_prob)\n",
    "    # Fully connected final output\n",
    "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "    \n",
    "  def forward(self, x, hidden):\n",
    "    '''\n",
    "    Forward pass through the network. x - Input, hidden is the cell state or hidden state.(memory).  \n",
    "    '''\n",
    "    # Output from LSTM\n",
    "    r_out, hidden = self.lstm(x, hidden)\n",
    "    out = self.dropout(r_out)\n",
    "    out = out.contiguous().view(-1,self.n_hidden)\n",
    "    out = self.fc(out)\n",
    "    return out, hidden\n",
    "  \n",
    "  def init_hidden(self, batch_size):\n",
    "    '''\n",
    "    Initialize hidden state. \n",
    "    '''\n",
    "    weight = next(self.parameters()).data\n",
    "    if (gpu_is):\n",
    "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "              weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "    else:\n",
    "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "    return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2s1HpxowOYE"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "def train(net, data, epochs = 10, batch_size = 10, seq_length = 50, lr = 0.001, clip = 5, val_frac = 0.2, print_every=15):\n",
    "  '''\n",
    "  Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss.\n",
    "  '''\n",
    "  net.train()\n",
    "  opt = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  # training and validation data\n",
    "  val_idx = int(len(data) * (1 - val_frac))\n",
    "  data, val_data = data[:val_idx], data[val_idx:]\n",
    "  \n",
    "  if (gpu_is):\n",
    "    net.cuda()\n",
    "  \n",
    "  counter = 0\n",
    "  n_chars = len(net.chars)\n",
    "  for e in range(epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    for x,y in get_batches(data, batch_size, seq_length):\n",
    "      counter += 1\n",
    "      x = one_hot_encode(x, n_chars)\n",
    "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "      if (gpu_is):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "      # Creating new variable for the hidden state.\n",
    "      # otherwise it would back propogate through the entire history\n",
    "      h = tuple([each.data for each in h])\n",
    "      \n",
    "      # Zero accumulated Gradients\n",
    "      net.zero_grad()\n",
    "      \n",
    "      output, h = net.forward(inputs, h)\n",
    "      loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
    "      loss.backward()\n",
    "      \n",
    "      # Clip gradients to overcome the exploding gradient problem in RNNs\n",
    "      nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "      opt.step()\n",
    "      # LOSS STATS\n",
    "      if counter % print_every == 0:\n",
    "        net.eval()\n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        for x,y in get_batches(val_data, batch_size, seq_length):\n",
    "          x = one_hot_encode(x, n_chars)\n",
    "          inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "          if (gpu_is):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "          val_h = tuple([each.data for each in val_h])\n",
    "          output, val_h = net.forward(inputs, val_h)\n",
    "          val_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
    "          val_losses.append(val_loss.item())\n",
    "          \n",
    "        net.train()\n",
    "        print(\"Epoch: {}/{} ...\".format(e+1, epochs),\n",
    "              \"Step: {} ...\".format(counter),\n",
    "              \"Loss: {:.4f} ...\".format(loss.item()),\n",
    "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "          \n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "InmvqMMCuDli",
    "outputId": "6e30f1a8-9de1-4ea2-9767-1420e6dbba4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.4)\n",
      "  (dropout): Dropout(p=0.4)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "dropout = 0.4\n",
    "net = CharRNN(chars, n_hidden, n_layers,dropout)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FM1Dcz6nWyF6"
   },
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 874
    },
    "colab_type": "code",
    "id": "OAIhkjb1I1oA",
    "outputId": "67678f75-4a20-4a14-d7b7-754bc8f0b43d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 ... Step: 50 ... Loss: 3.1179 ... Val Loss: 3.1165\n",
      "Epoch: 1/20 ... Step: 100 ... Loss: 2.9335 ... Val Loss: 2.9379\n",
      "Epoch: 2/20 ... Step: 150 ... Loss: 2.5052 ... Val Loss: 2.4835\n",
      "Epoch: 2/20 ... Step: 200 ... Loss: 2.3007 ... Val Loss: 2.3130\n",
      "Epoch: 3/20 ... Step: 250 ... Loss: 2.1763 ... Val Loss: 2.1645\n",
      "Epoch: 3/20 ... Step: 300 ... Loss: 2.0873 ... Val Loss: 2.0495\n",
      "Epoch: 3/20 ... Step: 350 ... Loss: 1.9866 ... Val Loss: 1.9478\n",
      "Epoch: 4/20 ... Step: 400 ... Loss: 1.8681 ... Val Loss: 1.8628\n",
      "Epoch: 4/20 ... Step: 450 ... Loss: 1.8255 ... Val Loss: 1.7939\n",
      "Epoch: 5/20 ... Step: 500 ... Loss: 1.7616 ... Val Loss: 1.7325\n",
      "Epoch: 5/20 ... Step: 550 ... Loss: 1.7071 ... Val Loss: 1.6814\n",
      "Epoch: 5/20 ... Step: 600 ... Loss: 1.6754 ... Val Loss: 1.6415\n",
      "Epoch: 6/20 ... Step: 650 ... Loss: 1.6234 ... Val Loss: 1.6050\n",
      "Epoch: 6/20 ... Step: 700 ... Loss: 1.5930 ... Val Loss: 1.5691\n",
      "Epoch: 7/20 ... Step: 750 ... Loss: 1.5473 ... Val Loss: 1.5448\n",
      "Epoch: 7/20 ... Step: 800 ... Loss: 1.5285 ... Val Loss: 1.5188\n",
      "Epoch: 7/20 ... Step: 850 ... Loss: 1.4896 ... Val Loss: 1.4947\n",
      "Epoch: 8/20 ... Step: 900 ... Loss: 1.4880 ... Val Loss: 1.4787\n",
      "Epoch: 8/20 ... Step: 950 ... Loss: 1.4800 ... Val Loss: 1.4537\n",
      "Epoch: 9/20 ... Step: 1000 ... Loss: 1.4562 ... Val Loss: 1.4368\n",
      "Epoch: 9/20 ... Step: 1050 ... Loss: 1.4519 ... Val Loss: 1.4226\n",
      "Epoch: 9/20 ... Step: 1100 ... Loss: 1.4360 ... Val Loss: 1.4138\n",
      "Epoch: 10/20 ... Step: 1150 ... Loss: 1.4006 ... Val Loss: 1.3936\n",
      "Epoch: 10/20 ... Step: 1200 ... Loss: 1.3605 ... Val Loss: 1.3825\n",
      "Epoch: 11/20 ... Step: 1250 ... Loss: 1.3526 ... Val Loss: 1.3771\n",
      "Epoch: 11/20 ... Step: 1300 ... Loss: 1.3572 ... Val Loss: 1.3628\n",
      "Epoch: 11/20 ... Step: 1350 ... Loss: 1.3676 ... Val Loss: 1.3529\n",
      "Epoch: 12/20 ... Step: 1400 ... Loss: 1.3406 ... Val Loss: 1.3450\n",
      "Epoch: 12/20 ... Step: 1450 ... Loss: 1.3294 ... Val Loss: 1.3358\n",
      "Epoch: 13/20 ... Step: 1500 ... Loss: 1.3019 ... Val Loss: 1.3317\n",
      "Epoch: 13/20 ... Step: 1550 ... Loss: 1.3203 ... Val Loss: 1.3231\n",
      "Epoch: 13/20 ... Step: 1600 ... Loss: 1.2891 ... Val Loss: 1.3162\n",
      "Epoch: 14/20 ... Step: 1650 ... Loss: 1.2983 ... Val Loss: 1.3131\n",
      "Epoch: 14/20 ... Step: 1700 ... Loss: 1.3090 ... Val Loss: 1.3064\n",
      "Epoch: 15/20 ... Step: 1750 ... Loss: 1.2901 ... Val Loss: 1.3063\n",
      "Epoch: 15/20 ... Step: 1800 ... Loss: 1.2683 ... Val Loss: 1.2959\n",
      "Epoch: 15/20 ... Step: 1850 ... Loss: 1.2810 ... Val Loss: 1.2916\n",
      "Epoch: 16/20 ... Step: 1900 ... Loss: 1.2648 ... Val Loss: 1.2883\n",
      "Epoch: 16/20 ... Step: 1950 ... Loss: 1.2722 ... Val Loss: 1.2852\n",
      "Epoch: 17/20 ... Step: 2000 ... Loss: 1.2498 ... Val Loss: 1.2823\n",
      "Epoch: 17/20 ... Step: 2050 ... Loss: 1.2681 ... Val Loss: 1.2750\n",
      "Epoch: 17/20 ... Step: 2100 ... Loss: 1.2505 ... Val Loss: 1.2746\n",
      "Epoch: 18/20 ... Step: 2150 ... Loss: 1.2166 ... Val Loss: 1.2707\n",
      "Epoch: 18/20 ... Step: 2200 ... Loss: 1.2236 ... Val Loss: 1.2693\n",
      "Epoch: 19/20 ... Step: 2250 ... Loss: 1.2044 ... Val Loss: 1.2681\n",
      "Epoch: 19/20 ... Step: 2300 ... Loss: 1.2191 ... Val Loss: 1.2665\n",
      "Epoch: 19/20 ... Step: 2350 ... Loss: 1.2151 ... Val Loss: 1.2653\n",
      "Epoch: 20/20 ... Step: 2400 ... Loss: 1.2035 ... Val Loss: 1.2591\n",
      "Epoch: 20/20 ... Step: 2450 ... Loss: 1.1723 ... Val Loss: 1.2568\n",
      "CPU times: user 5min 11s, sys: 3min 58s, total: 9min 10s\n",
      "Wall time: 9min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTbFN8PKWvZ9"
   },
   "source": [
    "## 5. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HY_Z_eoQnm5"
   },
   "outputs": [],
   "source": [
    "model_name = 'char_rnn.net'\n",
    "checkpoint = {'n_hidden':net.n_hidden,\n",
    "             'n_layers':net.n_layers,\n",
    "             'state_dict':net.state_dict(),\n",
    "             'tokens':net.chars}\n",
    "with open(PATH + model_name, 'wb') as f:\n",
    "  torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgmVNbMBX_hP"
   },
   "source": [
    "## 6. Predict and generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wzOCYNZX397"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(gpu_is):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(gpu_is):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tt3mufo0YIEV"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(gpu_is):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # Prime word\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "5tjb4IUMYKqn",
    "outputId": "8a4e664d-731b-4965-8da0-f0cc9ac83339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna, he would not be as to be, tryented that the\n",
      "position was necessary on her for her fingers in her mother. And an her there were no posicion\n",
      "that he would\n",
      "say, and would be ten and\n",
      "support. And the doctor and wish alone to the stares without\n",
      "an incloved to him, with all the sard to her shame in a charm and hope, and took his shout that thought which he saw the childree and the\n",
      "man and he had to be talking of the subject of that serious tables and things with her shade of the men in his hand. The sound of the same time, and he wanted\n",
      "to take his former this part where his best to go on, and had so going, she could\n",
      "not say, the court how he would not take of him work in half, and with a stretch hore his eyes shirk watcing a little time when the conversation was the steps to see the\n",
      "proper stairs on another shame with\n",
      "surery\n",
      "and seemed in his baby and the\n",
      "point of him. He had to\n",
      "distract the conversation in the past of his hand, and had askong him with what he was the fear for a splanging,\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "qkFLR3rAYNL9",
    "outputId": "01f93fae-4bed-4119-f8ad-4f6ef9359b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.4)\n",
      "  (dropout): Dropout(p=0.4)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# LOAD model and check\n",
    "model_name = 'char_rnn.net'\n",
    "with open(PATH+model_name, 'rb') as f:\n",
    "  checkpoint = torch.load(f,map_location='cpu') # Remove map_location if using GPU.\n",
    "  \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'], drop_prob=0.4)\n",
    "loaded.load_state_dict(checkpoint['state_dict'])\n",
    "print(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "id": "dDPQrVWPZckJ",
    "outputId": "e956b557-44b9-4d70-ab73-8b434068dfd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boy asked him, and at the table, and his boots of the prestraiting\n",
      "close of her eyes the starried settless and the conversation, as he was\n",
      "striking at the creat of them in her mind a step to her hand and terrously before anything before the charm, which was\n",
      "at the childroo at\n",
      "that mamening--his son.\n",
      "\n",
      "\"I won't go to see.\"\n",
      "\n",
      "\"Why so and I have said anything,\" said Anna, smiling,\n",
      "walking away her head.\n",
      "\n",
      "\"You can't say that I can do nothing with me to think of my heart,\" he answered, and as she was at once time that she went up tightly\n",
      "by a second\n",
      "princess. She had been so that the mere sent of that with his soul, and he said something to\n",
      "him with a\n",
      "subder and take a finger and his, at a land would\n",
      "be true, and he was thinking to this since anything of the state of his feet. She did not\n",
      "stay her head to his story out of the steps and the same the princess, with her head again.\n",
      "\n",
      "Sergey Ivanovitch was a conviction what he saw happiness whether the stants steps, a peacant child, which was a look of the marsh and his hand\n",
      "to this\n",
      "moment of the conversation with all his sensition of her face, as he had been\n",
      "satisfied when they discurbed the\n",
      "country this\n",
      "sister, had\n",
      "already following him, but at all his baby should have sting his sense of the mare. He was to go, was that he was always fancied with his hand of this setulat to her, but there is no hand of this to that he, and that the clonder was a gentleman's father of that\n",
      "settle, who, the most\n",
      "possibility, and that that had been bound to the sound of the monty to brang him out\n",
      "the\n",
      "principles and his\n",
      "foread husband.\n",
      "\n",
      "\"I don't know\n",
      "her, and think of it. We, are that I'm not sincere and despoise that I am\n",
      "not anything. I'll say that you're so many...\"\n",
      "\n",
      "Sitting off it were on a back in his eyes.\n",
      "\n",
      "\"I'm so going, she is a converition is to be of all the coachman that I've said.\n",
      "I shall have a sort\n",
      "of children. I'm not time to speak of it. But it can't say them, and he\n",
      "did not know that it's necessary, that I can't\n",
      "understand a long while it \n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "# Helper functions required are...\n",
    "# predict, sample, one_hot_encode, Network architecture.\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"The boy asked\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3gnz8HNqZkmb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C_rnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
