{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ||-//\n",
    "**T0P**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YOaZnVzTrZDS"
   },
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMCzpFvkq5UM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "yswgwo7ureps",
    "outputId": "b2b9e114-624a-4c8f-b684-4f97e584c9d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Fall Away\"\n",
      "\n",
      "I don't wanna fall, fall away\n",
      "I don't wanna fall, fall away\n",
      "I'll keep the lights on in this place\n",
      "'Cause I don't wanna fall, fall away\n",
      "\n",
      "I don't wanna fall, fall away\n",
      "I don't, I don't wanna fall, fall away\n",
      "I will keep the lights on in this place\n",
      "'Cause I don't wanna fall, fall away\n",
      "\n",
      "I disguise\n",
      "And I will lie\n",
      "And I will take my precious time\n",
      "As the days melt away\n",
      "As I stand in line\n",
      "And I die as I wait as I wait on my crime\n",
      "And I'll try to delay what you make of my life\n",
      "But I don't want your way,\n",
      "I want mine\n",
      "I'm dying and I'm trying\n",
      "But believe me I'm fine\n",
      "But I'm lying,\n",
      "I'm so very far from fine\n",
      "\n",
      "And I, I can feel the pull begin\n",
      "Feel my conscience wearing thin\n",
      "And my skin\n",
      "It will start to break up and fall apart\n",
      "\n",
      "I don't wanna fall, fall away\n",
      "I don't wanna fall, fall away\n",
      "I will keep the lights on in this place\n",
      "'Cause I don't wanna fall, fall away\n",
      "\n",
      "Every time I feel selfish ambition\n",
      "Is taking my vision\n",
      "My crime is my sentence\n",
      "Repentance is taking commission\n",
      "It's taking a toll\n",
      "On my soul\n",
      "I'm screaming submission and,\n",
      "I don't know if I am dying or living\n",
      "'Cause I will save face\n",
      "For name's sake\n",
      "Abuse grace\n",
      "Take aim to obtain a new name\n",
      "And a newer place\n",
      "But my name is lame\n",
      "I can't walk and I ain't the same\n",
      "And my name became\n",
      "New destiny to the grave\n",
      "\n",
      "And I, I can feel the pull begin\n",
      "Feel my conscience wearing thin\n",
      "And my skin,\n",
      "It will start to break up and fall apart\n",
      "\n",
      "I don't wanna fall, fall away\n",
      "I don't wanna fall, fall away\n",
      "I wi\n"
     ]
    }
   ],
   "source": [
    "# Loading the nobel\n",
    "PATH = 'data/'\n",
    "with open(PATH+'top.txt' ,'r') as f:\n",
    "  text = f.read()\n",
    "\n",
    "print(text[535:2000]) # One of my favorites..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8W4IC0CtZsH"
   },
   "source": [
    "## 1. DATA PREP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "l4kUAquCroH-",
    "outputId": "ee59c7e6-9374-4f7b-94ba-ed250c554c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char2int:\n",
      "{'c': 0, ' ': 1, 'G': 2, 'j': 3, 't': 4, 'a': 5, 'A': 6, 'g': 7, 'k': 8, 'q': 9, 'M': 10, 'F': 11, '(': 12, 'E': 13, '2': 14, 'x': 15, '.': 16, 'T': 17, '1': 18, 'U': 19, '?': 20, 'd': 21, '0': 22, 'r': 23, 'L': 24, '7': 25, 'B': 26, 'p': 27, 'n': 28, 'P': 29, ',': 30, 'Z': 31, '-': 32, 'J': 33, '6': 34, \"'\": 35, 'S': 36, 's': 37, 'b': 38, '4': 39, 'D': 40, 'l': 41, ')': 42, '\\n': 43, '&': 44, 'i': 45, 'O': 46, 'R': 47, 'N': 48, 'z': 49, 'Q': 50, 'm': 51, 'u': 52, '!': 53, 'V': 54, '—': 55, '’': 56, 'o': 57, 'e': 58, 'Y': 59, '3': 60, 'h': 61, 'K': 62, 'I': 63, 'w': 64, 'W': 65, 'C': 66, '\\ufeff': 67, 'f': 68, '[': 69, ':': 70, 'H': 71, 'v': 72, 'é': 73, '\"': 74, 'y': 75, '8': 76, '5': 77, ']': 78}\n",
      "\n",
      "int2char:\n",
      "{0: 'c', 1: ' ', 2: 'G', 3: 'j', 4: 't', 5: 'a', 6: 'A', 7: 'g', 8: 'k', 9: 'q', 10: 'M', 11: 'F', 12: '(', 13: 'E', 14: '2', 15: 'x', 16: '.', 17: 'T', 18: '1', 19: 'U', 20: '?', 21: 'd', 22: '0', 23: 'r', 24: 'L', 25: '7', 26: 'B', 27: 'p', 28: 'n', 29: 'P', 30: ',', 31: 'Z', 32: '-', 33: 'J', 34: '6', 35: \"'\", 36: 'S', 37: 's', 38: 'b', 39: '4', 40: 'D', 41: 'l', 42: ')', 43: '\\n', 44: '&', 45: 'i', 46: 'O', 47: 'R', 48: 'N', 49: 'z', 50: 'Q', 51: 'm', 52: 'u', 53: '!', 54: 'V', 55: '—', 56: '’', 57: 'o', 58: 'e', 59: 'Y', 60: '3', 61: 'h', 62: 'K', 63: 'I', 64: 'w', 65: 'W', 66: 'C', 67: '\\ufeff', 68: 'f', 69: '[', 70: ':', 71: 'H', 72: 'v', 73: 'é', 74: '\"', 75: 'y', 76: '8', 77: '5', 78: ']'}\n",
      "\n",
      "Length:99648\n",
      "\n",
      "Encoded:\n",
      "[67 74 63 51 27 41 45  0 45  4  1 40 58 51  5 28 21  1 11 57 23  1 29 23\n",
      " 57 57 68 74 43 43 63  1  8 28 57 64  1 75 57 52 35 23 58  1 28 57  4  1\n",
      "  5  1]\n",
      "\n",
      "\n",
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "chars = tuple(set(text))\n",
    "# Dictionaries for mapping characters to integers and vice versa.\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {c:i for i,c in int2char.items()}\n",
    "# Encode entire text to numbers i.e each character with its numerical value.\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "print('char2int:\\n{}\\n\\nint2char:\\n{}\\n\\nLength:{}\\n\\nEncoded:\\n{}\\n\\n'.format(char2int,int2char,len(encoded),encoded[:50]))\n",
    "\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5qRcHQEu8vn"
   },
   "source": [
    "## 2. Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqSZL3jctsTG"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "  '''\n",
    "  Generator which returns batch of size : batch_size*seq_length\n",
    "  '''\n",
    "  chars_batch = batch_size * seq_length\n",
    "  n_batches = len(arr) // chars_batch\n",
    "  \n",
    "  # Make only full batches\n",
    "  arr = arr[: n_batches * chars_batch]\n",
    "  # Resize the array\n",
    "  arr = arr.reshape((batch_size,-1))\n",
    "  # Get Batches\n",
    "  for i in range(0, arr.shape[1], seq_length):\n",
    "    # Features\n",
    "    x = arr[:, i:i + seq_length]\n",
    "    # Targets shifted by One in future\n",
    "    y = np.zeros_like(x)\n",
    "    try:\n",
    "      y[:,:-1], y[:,-1] = x[:,1:], arr[:,i+seq_length]\n",
    "    except IndexError:\n",
    "      y[:,:-1], y[:,-1] = x[:,1:], arr[:,0]\n",
    "    \n",
    "    yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "colab_type": "code",
    "id": "HJLZHwKMvDsN",
    "outputId": "d396416e-d722-46e4-93c2-b7cacc9ec3fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (8, 50)\n",
      "x\n",
      " [[67 74 63 51 27 41 45  0 45  4  1 40 58 51  5 28 21  1 11 57 23  1 29 23\n",
      "  57 57 68 74 43 43 63  1  8 28 57 64  1 75 57 52 35 23 58  1 28 57  4  1\n",
      "   5  1]\n",
      " [28 57  4  1 51 75  1 57 64 28 43 63  1  8 28 58 64  1 51 75  1 37  8 45\n",
      "  28  1  4 61  5  4  1 64 23  5 27 27 58 21  1 51 75  1 68 23  5 51 58 43\n",
      "  65  5]\n",
      " [51 58  1 57 68 43 65 61 57  1 63  1  8 45 41 41 58 21  1 45 28 37 45 21\n",
      "  58  1 51 75  1 21 23 58  5 51 43 63  1 61  5  4 58  1  4 61 45 37  1  0\n",
      "   5 23]\n",
      " [57  1 37  5 75  1  7 57 57 21 38 75 58 30  1 21 58 68 75 30  1  5 28 21\n",
      "   1 21 58 28 75 30 43 65 61  5  4  1 45  4  1 45 37  1 63 35 51  1 68 58\n",
      "   5 23]\n",
      " [ 5 28 28  5  1  0  5 41 41  1 75 57 52  1 45 28  1  4 61 58  1 28 45  7\n",
      "  61  4  1  4 45 51 58 43 40 57 28 35  4  1 64  5 28 28  5  1  7 45 72 58\n",
      "   1 75]\n",
      " [ 1 37 51 45 41 58 30  1  0 52  4  1 51 58  1 68  5 23  4 61 58 23 30 43\n",
      "  17 61  5 28  1 63 35 72 58  1 58 72 58 23  1 38 58 58 28 16 43 43 17 61\n",
      "   5 28]\n",
      " [30 43 36  4  5 75 45 28  7  1 45 28  1  4 61 58  1 37  5 51 58  1 23 57\n",
      "  57 51  1 63  1 64  5 37  1 38 57 23 28  1 45 28 30 43 63  1 41 57 57  8\n",
      "   1 57]\n",
      " [45  7 61 58 37  4  1 57 68  1 27 23  5 45 37 58 30  1  5 28 21  1 61  5\n",
      "  28  7  1  4 61 58 45 23  1 38  5 28 28 58 23  1 68 23 57 51  1  5  1  0\n",
      "  58 45]]\n",
      "\n",
      "y\n",
      " [[74 63 51 27 41 45  0 45  4  1]\n",
      " [57  4  1 51 75  1 57 64 28 43]\n",
      " [58  1 57 68 43 65 61 57  1 63]\n",
      " [ 1 37  5 75  1  7 57 57 21 38]\n",
      " [28 28  5  1  0  5 41 41  1 75]\n",
      " [37 51 45 41 58 30  1  0 52  4]\n",
      " [43 36  4  5 75 45 28  7  1 45]\n",
      " [ 7 61 58 37  4  1 57 68  1 27]]\n"
     ]
    }
   ],
   "source": [
    "# TEST RUN\n",
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n",
    "# printing out the first 10 items in a sequence\n",
    "print('Shape', x.shape)\n",
    "print('x\\n',x)\n",
    "print('\\ny\\n', y[:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Pa0hQPVxvGzF",
    "outputId": "c246fbd0-47e2-4a30-c036-caddf2249bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "# GPU check\n",
    "gpu_is = torch.cuda.is_available()\n",
    "if(gpu_is):\n",
    "  print('Training on GPU')\n",
    "else:\n",
    "  print('NO GPU | Training on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a2k-LAuWvPDb"
   },
   "source": [
    "## 3. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qD2XQ_UYvLP5"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "  def __init__(self, tokens, n_hidden = 256, n_layers = 2, drop_prob = 0.3, lr = 0.001):\n",
    "    super().__init__()\n",
    "    self.drop_prob = drop_prob\n",
    "    self.n_hidden = n_hidden\n",
    "    self.n_layers = n_layers\n",
    "    self.lr = lr\n",
    "    \n",
    "    # Character dictionaries\n",
    "    self.chars = tokens\n",
    "    self.int2char = dict(enumerate(self.chars))\n",
    "    self.char2int = {c:i for i,c in self.int2char.items()}\n",
    "    \n",
    "    # LSTM \n",
    "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout = drop_prob, batch_first = True)\n",
    "    # Dropout\n",
    "    self.dropout = nn.Dropout(drop_prob)\n",
    "    # Fully connected final output\n",
    "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "    \n",
    "  def forward(self, x, hidden):\n",
    "    '''\n",
    "    Forward pass through the network. x - Input, hidden is the cell state or hidden state.(memory).  \n",
    "    '''\n",
    "    # Output from LSTM\n",
    "    r_out, hidden = self.lstm(x, hidden)\n",
    "    out = self.dropout(r_out)\n",
    "    out = out.contiguous().view(-1,self.n_hidden)\n",
    "    out = self.fc(out)\n",
    "    return out, hidden\n",
    "  \n",
    "  def init_hidden(self, batch_size):\n",
    "    '''\n",
    "    Initialize hidden state. \n",
    "    '''\n",
    "    weight = next(self.parameters()).data\n",
    "    if (gpu_is):\n",
    "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "              weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "    else:\n",
    "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqkqLhkKvZDu"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "def train(net, data, epochs = 10, batch_size = 10, seq_length = 50, lr = 0.001, clip = 5, val_frac = 0.2, print_every=15):\n",
    "  '''\n",
    "  Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss.\n",
    "  '''\n",
    "  net.train()\n",
    "  opt = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  # training and validation data\n",
    "  val_idx = int(len(data) * (1 - val_frac))\n",
    "  data, val_data = data[:val_idx], data[val_idx:]\n",
    "  \n",
    "  if (gpu_is):\n",
    "    net.cuda()\n",
    "  \n",
    "  counter = 0\n",
    "  n_chars = len(net.chars)\n",
    "  for e in range(epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    for x,y in get_batches(data, batch_size, seq_length):\n",
    "      counter += 1\n",
    "      x = one_hot_encode(x, n_chars)\n",
    "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "      if (gpu_is):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "      # Creating new variable for the hidden state.\n",
    "      # otherwise it would back propogate through the entire history\n",
    "      h = tuple([each.data for each in h])\n",
    "      \n",
    "      # Zero accumulated Gradients\n",
    "      net.zero_grad()\n",
    "      \n",
    "      output, h = net.forward(inputs, h)\n",
    "      loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
    "      loss.backward()\n",
    "      \n",
    "      # Clip gradients to overcome the exploding gradient problem in RNNs\n",
    "      nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "      opt.step()\n",
    "      # LOSS STATS\n",
    "      if counter % print_every == 0:\n",
    "        net.eval()\n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        for x,y in get_batches(val_data, batch_size, seq_length):\n",
    "          x = one_hot_encode(x, n_chars)\n",
    "          inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "          if (gpu_is):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "          val_h = tuple([each.data for each in val_h])\n",
    "          output, val_h = net.forward(inputs, val_h)\n",
    "          val_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
    "          val_losses.append(val_loss.item())\n",
    "          \n",
    "        net.train()\n",
    "        print(\"Epoch: {}/{} ...\".format(e+1, epochs),\n",
    "              \"Step: {} ...\".format(counter),\n",
    "              \"Loss: {:.4f} ...\".format(loss.item()),\n",
    "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1Y1VZ7nyYp0"
   },
   "outputs": [],
   "source": [
    "#del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "EYR1vWo7vdD7",
    "outputId": "639c584d-cdb5-47ed-c2b6-3a8e5ca1cacd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(79, 512, num_layers=2, batch_first=True, dropout=0.4)\n",
      "  (dropout): Dropout(p=0.4)\n",
      "  (fc): Linear(in_features=512, out_features=79, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "dropout = 0.4\n",
    "net = CharRNN(chars, n_hidden, n_layers,dropout)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I93hKhLbw-OR"
   },
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "DwVGsXVs3Jrx",
    "outputId": "bc51e07f-857e-42bc-a31d-f2ad47fb7855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff\"Implicit Demand For Proof\"', '', \"I know you're not a liar\", 'And I know you could set fire']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({0: 694,\n",
       "         1: 105,\n",
       "         2: 151,\n",
       "         3: 275,\n",
       "         4: 376,\n",
       "         5: 417,\n",
       "         6: 387,\n",
       "         7: 371,\n",
       "         8: 308,\n",
       "         9: 212,\n",
       "         10: 205,\n",
       "         11: 111,\n",
       "         12: 92,\n",
       "         13: 54,\n",
       "         14: 22,\n",
       "         15: 6,\n",
       "         16: 1,\n",
       "         17: 1,\n",
       "         18: 3})"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "x = text.split('\\n')\n",
    "print(x[:4])\n",
    "c = Counter([len(i.split()) for i in x])\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "iUJJvUYUviGl",
    "outputId": "8bfa9ea5-7b28-4d8e-e502-dd68feb908e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 ... Step: 30 ... Loss: 3.2308 ... Val Loss: 3.2144\n",
      "Epoch: 2/20 ... Step: 60 ... Loss: 3.1708 ... Val Loss: 3.2016\n",
      "Epoch: 2/20 ... Step: 90 ... Loss: 3.1653 ... Val Loss: 3.1778\n",
      "Epoch: 3/20 ... Step: 120 ... Loss: 2.9840 ... Val Loss: 3.0152\n",
      "Epoch: 3/20 ... Step: 150 ... Loss: 2.6626 ... Val Loss: 2.7172\n",
      "Epoch: 4/20 ... Step: 180 ... Loss: 2.4886 ... Val Loss: 2.5503\n",
      "Epoch: 4/20 ... Step: 210 ... Loss: 2.4028 ... Val Loss: 2.4256\n",
      "Epoch: 5/20 ... Step: 240 ... Loss: 2.2858 ... Val Loss: 2.3386\n",
      "Epoch: 6/20 ... Step: 270 ... Loss: 2.1894 ... Val Loss: 2.2874\n",
      "Epoch: 6/20 ... Step: 300 ... Loss: 2.1222 ... Val Loss: 2.2424\n",
      "Epoch: 7/20 ... Step: 330 ... Loss: 2.0607 ... Val Loss: 2.1961\n",
      "Epoch: 7/20 ... Step: 360 ... Loss: 2.0016 ... Val Loss: 2.1524\n",
      "Epoch: 8/20 ... Step: 390 ... Loss: 1.9007 ... Val Loss: 2.1282\n",
      "Epoch: 8/20 ... Step: 420 ... Loss: 1.9354 ... Val Loss: 2.0976\n",
      "Epoch: 9/20 ... Step: 450 ... Loss: 1.8613 ... Val Loss: 2.0837\n",
      "Epoch: 10/20 ... Step: 480 ... Loss: 1.8436 ... Val Loss: 2.0533\n",
      "Epoch: 10/20 ... Step: 510 ... Loss: 1.6922 ... Val Loss: 2.0395\n",
      "Epoch: 11/20 ... Step: 540 ... Loss: 1.7969 ... Val Loss: 2.0170\n",
      "Epoch: 11/20 ... Step: 570 ... Loss: 1.7253 ... Val Loss: 2.0168\n",
      "Epoch: 12/20 ... Step: 600 ... Loss: 1.6551 ... Val Loss: 2.0008\n",
      "Epoch: 12/20 ... Step: 630 ... Loss: 1.6546 ... Val Loss: 1.9878\n",
      "Epoch: 13/20 ... Step: 660 ... Loss: 1.5618 ... Val Loss: 2.0002\n",
      "Epoch: 14/20 ... Step: 690 ... Loss: 1.6910 ... Val Loss: 1.9843\n",
      "Epoch: 14/20 ... Step: 720 ... Loss: 1.4899 ... Val Loss: 1.9880\n",
      "Epoch: 15/20 ... Step: 750 ... Loss: 1.4208 ... Val Loss: 1.9835\n",
      "Epoch: 15/20 ... Step: 780 ... Loss: 1.5250 ... Val Loss: 1.9880\n",
      "Epoch: 16/20 ... Step: 810 ... Loss: 1.3852 ... Val Loss: 1.9641\n",
      "Epoch: 16/20 ... Step: 840 ... Loss: 1.4019 ... Val Loss: 2.0205\n",
      "Epoch: 17/20 ... Step: 870 ... Loss: 1.2644 ... Val Loss: 1.9859\n",
      "Epoch: 17/20 ... Step: 900 ... Loss: 1.3163 ... Val Loss: 1.9825\n",
      "Epoch: 18/20 ... Step: 930 ... Loss: 1.1540 ... Val Loss: 2.0117\n",
      "Epoch: 19/20 ... Step: 960 ... Loss: 1.2962 ... Val Loss: 2.0113\n",
      "Epoch: 19/20 ... Step: 990 ... Loss: 1.1958 ... Val Loss: 2.0328\n",
      "Epoch: 20/20 ... Step: 1020 ... Loss: 1.1540 ... Val Loss: 2.0470\n",
      "Epoch: 20/20 ... Step: 1050 ... Loss: 1.2212 ... Val Loss: 2.0431\n",
      "CPU times: user 26.7 s, sys: 16.3 s, total: 42.9 s\n",
      "Wall time: 43.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 50\n",
    "seq_length = 30\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "By7AYMirzVyd"
   },
   "source": [
    "## 5. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evddOO7AxVuS"
   },
   "outputs": [],
   "source": [
    "model_name = 'top_rnn.net'\n",
    "checkpoint = {'n_hidden':net.n_hidden,\n",
    "             'n_layers':net.n_layers,\n",
    "             'state_dict':net.state_dict(),\n",
    "             'tokens':net.chars}\n",
    "with open(PATH + model_name, 'wb') as f:\n",
    "  torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49_Va3tyzdKQ"
   },
   "source": [
    "## 6. Predict and generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gywTIUAHzbiH"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(gpu_is):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(gpu_is):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xgkv26jMzkL9"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(gpu_is):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "colab_type": "code",
    "id": "6BJMSa7LznJn",
    "outputId": "88c50c2b-ff6b-42b2-a105-88ad1a534d70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life is all about that\n",
      "I want to bryak your lease, somebodysw\n",
      "Is wasted and I hive knows head,\n",
      "There's so ngore.\n",
      "\n",
      "[2x:]\n",
      "We're someon with it\n",
      "I wannad\n",
      "And the know\n",
      "\n",
      "I'm strying to say, \"I wanna go la-da la-da da\n",
      "And oh\n",
      "No nat a beat and from?\n",
      "Your stall insine this to sad\n",
      "Will see a pollent to save me.\"\n",
      "\n",
      "Helpers of my dieds, three a moment, threes, can your, niget\n",
      "Taking in the ploud, on the mun.\n",
      "\n",
      "Na na na, oh oh\n",
      "Na na na, oh oh\n",
      "Na care no boh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh [2x]\n",
      "\n",
      "I'm not scared is a butter, play that fall the pust\n",
      "And the currar our becine and seed\n",
      "And this think stirn\n",
      "I callred,\n",
      "\n",
      "I don't wanna bely thinking\n",
      "\n",
      "And I walk all its treem,\n",
      "But I'm trying to bleer to stay, but now I can readle\n",
      "In this hese\n",
      "\n",
      "Yau and that I know,\n",
      "That we'll side your like mesing onf like it,\n",
      "I'm saying, something will the san I will free\n",
      "\n",
      "I'm not this mive there my nide,\n",
      "You are the ofly one is caling,\n",
      "Trost on my fitaling,\n",
      "I don't know that we're dound on the sure\n",
      "My, hore our haves to see alang,\n",
      "Thr\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='life is all abo', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-vLc0L-0R6d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9UBo0aFzrfU"
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Dataloader and shuffle.       \n",
    "# Yeild fuction - no shuffle...\n",
    "# Batch size and length as it is different -- window and stride.\n",
    "# MUSIC notes... that would be good."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "T0P.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
